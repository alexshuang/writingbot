![](https://upload-images.jianshu.io/upload_images/13575947-0cf594843461afd4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## æœºå™¨äººå†™æ‰‹çš„æ—¶ä»£æ¥ä¸´

ä½ å¯èƒ½å¬è¯´è¿‡ï¼Œ2015å¹´è…¾è®¯æ¨å‡ºäº†ä¸€ä¸ªå«Dreamwriterçš„è‡ªåŠ¨åŒ–æ–°é—»å†™ä½œæœºå™¨äººï¼Œå®ƒçš„æˆæ–‡é€Ÿåº¦ä»…ä¸º0.5ç§’ï¼Œä¸€çœ¨çœ¼åŠŸå¤«å°±èƒ½äº§å‡ºåå‡ ç¯‡æ–‡ç« ï¼Œè¿™æ˜¯ç›¸å½“æƒŠäººçš„ã€‚

ç°é˜¶æ®µçš„å†™ä½œæœºå™¨äººå¤§å¤šåº”ç”¨äºä½“è‚²å’Œè´¢ç»æ–°é—»ã€‚Why? ä¸€æ–¹é¢æ˜¯å› ä¸ºè¿™äº›é¢†åŸŸåŒ…å«å¤§é‡æ•°æ®èµ·ä¼ï¼Œæ¯”å¦‚è‚¡ä»·æ³¢åŠ¨ã€æ¯”èµ›æˆç»©ç­‰ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯¹äºè¿™ç±»æŠ¥é“ï¼Œè¯»è€…æ›´å…³å¿ƒçš„æ˜¯å…·ä½“æ•°å­—è€Œéè¯­è¨€æœ¬èº«ï¼Œä»–ä»¬å¯¹å¤æ¿çš„æ–°é—»å¥—è¯å·²æœ‰è¾ƒé«˜çš„å®¹å¿åº¦ã€‚

æˆ‘ä»¬è¦å¼€å‘çš„å†™ä½œæœºå™¨äººï¼Œæ˜¯é€šè¿‡è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰æ¥å­¦ä¹ è‹±æ–‡å½±è¯„å†™ä½œçš„ã€‚ä»¥ä¸‹ä¸¤æ®µå¯¹è¯åˆ†åˆ«æ˜¯æˆ‘å’Œæœºå™¨äººå†™çš„å½±è¯„ï¼Œå¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œæœºå™¨äººï¼ˆBotï¼‰å†™çš„ä¸œè¥¿è™½ç„¶ç¬¨æ‹™ï¼Œä½†å¥½åœ¨ä¸»é¢˜æ˜ç¡®ï¼Œæ²¡æœ‰è¯­æ³•ç¡¬ä¼¤ï¼Œçœ‹èµ·æ¥å†™çš„æœ‰æ¨¡æœ‰æ ·ï¼Œæœ‰ç‚¹å„¿å°å­¦ç”Ÿä½œæ–‡çš„å‘³é“ğŸ˜„ã€‚

> **\>**  It's not a movie, it's a video comedy. A lot of trash and a lot of humor.
**Bot:**  i'm sure you will be disappointed. the only thing that i didn't like was the fact that the movie wasn't funny.

>**\>**  God is really the reversal of the goose bumps
**Bot:**  i think that the movie is a bit too long, but it is a very good movie. i have to say that i was surprised to see that this movie was made in the same vein as "the blair witch project". 

å®Œæ•´çš„ä»£ç è§ï¼š [Notebook](https://github.com/alexshuang/writingbot/blob/master/writingbot.ipynb) /  [Github](https://github.com/alexshuang/writingbot)ã€‚

---

## [IMDB Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)

[IMDB](https://baike.baidu.com/item/%E4%BA%92%E8%81%94%E7%BD%91%E7%94%B5%E5%BD%B1%E8%B5%84%E6%96%99%E5%BA%93/1484580?fromtitle=IMDB&fromid=925061&fr=aladdin)æ˜¯ç±»ä¼¼å›½å†…çš„è±†ç“£ç”µå½±çš„ç½‘ç«™ï¼Œé‡Œé¢æœ‰å¤§é‡ç”¨æˆ·çš„è§‚å½±è¯„è®ºã€‚æˆ‘ä»¬è¦ç”¨çš„æ˜¯æ–¯å¦ç¦å¤§å­¦çš„[NLPæƒ…ç»ªåˆ†ææ•°æ®é›†](http://ai.stanford.edu/~amaas/data/sentiment/)ï¼Œå®ƒæ”¶å½•æœ‰100kæ¡IMDBå½±è¯„ã€‚

```
trn_txts = read_text(TRN_PATH)
val_txts = read_text(VAL_PATH)
txts = np.concatenate([trn_txts, val_txts])
df = pd.DataFrame({'text': txts})
```

æ•°æ®é›†ä¸­çš„å½±è¯„æ˜¯é€æ¡ä»¥æ–‡æœ¬æ–‡ä»¶çš„æ ¼å¼å­˜å‚¨çš„ï¼Œå› æ­¤ï¼Œéœ€è¦å…ˆæŠŠå®ƒä»¬å…¨éƒ¨è¯»å–æ±‡æ€»èµ·æ¥ï¼Œç”Ÿæˆpanda's dataframeï¼Œä¾¿äºåç»­å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚

### Preprocess: Tokenize & Numericalize

æˆ‘ä»¬çŸ¥é“ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å…¥åªèƒ½æ˜¯æ•°å€¼ï¼ˆint/long/floatï¼‰ï¼Œå› æ­¤ï¼Œæ–‡æœ¬é¢„å¤„ç†ä¸­æœ€ä¸»è¦çš„å·¥ä½œå°±æ˜¯å°†æ–‡å­—ï¼ˆstrï¼‰è½¬æ¢ä¸ºæ•°å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹å°±ç§°ä¸ºæ•°å€¼åŒ–ï¼ˆnumericalizeï¼‰ã€‚

æ•°å€¼åŒ–çš„å‰ææ˜¯å¥å­ï¼ˆsentenceï¼‰å·²ç»è¢«æ‹†åˆ†æˆç‰‡æ®µï¼ˆtokenï¼‰ã€‚ä¾‹å¦‚ï¼Œâ€œgood morning!â€è¿™ä¸ªå¥å­ï¼Œå‡è®¾ä»¥å•è¯ä¸ºå•ä½æ‹†åˆ†ï¼Œå°†å¾—åˆ° ["good", " ", "morning", "!"] tokenåºåˆ—ã€‚è¿™ä¸ªè¿‡ç¨‹å°±ç§°ä¸ºtokenizeã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆå¯¹æ•°æ®é›†çš„æ‰€æœ‰æ–‡æœ¬åštokenizeï¼Œè¿™éœ€è¦ç”¨åˆ°Tokenizerç±»ã€‚

```
class Tokenizer():
  def __init__(self, lang='en'):
    self.tok = spacy.load(lang)

    ......
  @staticmethod
  def tokenize_mp(x, n_cpus=None, lang='en'):
    ......
    with ThreadPoolExecutor(n_cpus) as e:
      toks += sum(e.map(Tokenizer(lang).tokenize, xs), [])
    return toks

  @staticmethod
  def tokenize_df(dl, label, lang='en'):
    ts = []
    for i, df in enumerate(dl):
      ts += Tokenizer.tokenize_mp(df.iloc[:, label].values, lang=lang)
    return ts
```
Tokenizerï¼Œå®ƒå°è£…äº†tokenizeå·¥å…·--spacyï¼Œå¹¶æ”¯æŒå¤šçº¿ç¨‹å¤„ç†ï¼ˆtokenize_mpï¼‰ã€‚NLPæ•°æ®é›†çš„ç‰¹ç‚¹æ˜¯æ•°æ®é‡åºå¤§ï¼ˆæœ¬ä¾‹çš„å•è¯æ•°è¶…è¿‡2400ä¸‡ï¼‰ï¼Œå› æ­¤ï¼Œå®ƒä»¬çš„é¢„å¤„ç†å°±å¯¹**CPUç®—åŠ›**å’Œ**å†…å­˜å¤§å°**æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚

```
chunksize = 25000
dl = pd.read_csv(PATH/'txts.csv', header=None, chunksize=chunksize)
ts = Tokenizer.tokenize_df(dl, 0)
```

é™¤äº†CPUç®—åŠ›ï¼Œ**å†…å­˜ä¸è¶³**åˆ™æ˜¯å¦ä¸€ä¸ªè®©NLPerå¤´ç—›ä¸å·²çš„éš¾é¢˜ã€‚æˆ‘ä»¬è¿™é‡Œé‡‡ç”¨åˆ†æ­¥åŠ è½½æ•°æ®é›†çš„æ–¹å¼æ¥é¿å…å†…å­˜ä¸è¶³ï¼Œpd.read_csv()ä¸ç›´æ¥è¯»å–æ•°æ®ï¼Œè€Œæ˜¯ç”Ÿæˆåˆ†å—ï¼ˆchunksizeï¼‰è¯»å–æ•°æ®çš„generatorè¿­ä»£å™¨ã€‚

```
freq = Counter(p for o in trn_toks for p in o)
freq.most_common(5)

[('the', 1208826),
 (',', 986530),
 ('.', 985698),
 ('and', 587158),
 ('a', 583198)]

vocab_size = 60000
min_freq = 4
itos = [n for n, v in freq.most_common(vocab_size) if v >= min_freq]
itos.insert(0, '_pad_')
itos.insert(0, '_unk_')
stoi = collections.defaultdict(lambda: 0, {o:i for i, o in enumerate(itos)})
```

tokenizeå®Œæˆä¹‹åï¼Œå°±è¦å¯¹æ•°æ®é›†çš„æ–‡æœ¬å†…å®¹åšnumericalizeã€‚numericalizeçš„è¿‡ç¨‹å°±æœ‰ç‚¹åƒè°æˆ˜å‰§ä¸­æƒ…æŠ¥å‘˜è§£ç å¯†æ–‡çš„æ¡¥æ®µï¼Œæƒ…æŠ¥å‘˜æ”¶åˆ°ä¸€ä¸²æ•°å­—æˆ–ä¹±ç ï¼Œè¿å¿™æå‡ºå¯†ç æœ¬ï¼Œé€ä¸ªæŸ¥æ‰¾å¯†æ–‡å¯¹åº”çš„æ˜æ–‡ã€‚

numbericalizeå¯¹tokenåºåˆ—ç¼–ç çš„è¿‡ç¨‹ä¹Ÿéœ€è¦è¿™æ ·ä¸€æœ¬â€œå¯†ç æœ¬â€ -- tokenå­—å…¸ï¼ˆstoiï¼‰ï¼Œå®ƒè®°å½•äº†æ‰€æœ‰å‡ºç°4æ¬¡ä»¥ä¸Šçš„é«˜é¢‘tokenä»¥åŠå®ƒä»¬å¯¹åº”çš„æ•°å€¼ï¼Œå­—å…¸æœ€å¤§é•¿åº¦æ˜¯60000ï¼Œé‚£äº›ä¸å­˜åœ¨äºå­—å…¸ä¸­çš„tokenä¼šè¢«æ ‡è®°ä¸ºunknownï¼ˆ\_unk\_ï¼‰ã€‚

### Dataset

é™¤äº†é¢„å¤„ç†ä¹‹å¤–ï¼Œæ–‡æœ¬åºåˆ—è¿˜éœ€è¦è½¬æ¢ä¸ºRNNéœ€è¦çš„ç»“æ„ï¼š

![Figure 1](https://upload-images.jianshu.io/upload_images/13575947-bf55caaa1ea7c1a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

å¦‚Figure1æ‰€ç¤ºï¼Œè¯åºåˆ—è¢«åˆ’åˆ†ä¸º**batch_size**ä¸ªæ®µï¼Œè¿™äº›æ®µç»è¿‡å åŠ ç¿»è½¬åå¾—åˆ°ä¸€ä¸ªçŸ©é˜µï¼Œè€Œmini-batchå°±æ˜¯çŸ©é˜µä¸­**bptt**ä¸ªè¿ç»­è¡Œç»„æˆçš„å­çŸ©é˜µã€‚

RNN inputï¼ˆmini-batchï¼‰ shapeæ˜¯**[seq_len, batch, input_size]**ï¼š**seq_len**æ˜¯mini-batchçš„è¡Œæ•°--BPTTï¼ˆBack Propagation Through Timeï¼‰ï¼Œ**batch**æ˜¯batch_sizeï¼Œ**input_size**æ˜¯è¯å‘é‡é•¿åº¦ã€‚

Figure1ä¸­çš„mini-batchå¹¶æ²¡æœ‰â€œ**input_size**â€è¿™ä¸ªç»´åº¦ï¼Œé‚£ä¹ˆï¼Œè¯å‘é‡æ˜¯ä»ä½•è€Œæ¥çš„ï¼Ÿç¥ç»ç½‘ç»œä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå®ƒå¾ˆéš¾é€šè¿‡è§‚å¯Ÿä¸€ä¸ªæ•°å­—å°±èƒ½å­¦ä¼šæ•°å­—èƒŒåä»£è¡¨çš„å•è¯ã€å•è¯é—´çš„å…³è”ä»¥åŠè¯­æ³•ç»“æ„ã€‚

ä¾‹å¦‚â€œ fridayâ€ã€"saturday"å’Œ"sunday"è¿™ä¸‰ä¸ªå•è¯ï¼Œå®ƒä»¬çš„ç¼–ç åˆ†åˆ«æ˜¯5ã€6å’Œ100ï¼Œä½ å¾ˆéš¾æƒ³è±¡å¦‚ä½•é€šè¿‡è§‚å¯Ÿè¿™äº›æ•°å­—æ¥å­¦ä¹ è¿™äº›è¯çš„æ„æ€ï¼Œä¸èƒ½å› ä¸º5å’Œ6ç›¸é‚»å°±åˆ¤å®šæ˜ŸæœŸäº”å’Œæ˜ŸæœŸå…­çš„å…³è”åº¦æœ€å¼ºã€‚å®é™…ä¸Šï¼Œæ¯ä¸ªå•è¯éƒ½æœ‰å¾ˆå¤šæ˜ç‰¹å¾å’Œæš—ç‰¹å¾ï¼Œä¾‹å¦‚"saturday"çš„æ˜ç‰¹å¾æ˜¯â€œå‘¨æœ«â€ï¼Œæš—ç‰¹å¾æ˜¯â€œè´­ç‰©â€ã€â€œå¨±ä¹â€ã€â€œç¤¾äº¤â€ç­‰ã€‚

æ—¢ç„¶ä¸€ä¸ªæ•°ä¸å¤Ÿï¼Œé‚£å°±ç”¨ä¸€ä¸ªå‘é‡çš„æ•°æ¥è¡¨å¾ä¸€ä¸ªè¯å§ã€‚è¿™ä¸ªå‘é‡å°±ç§°ä¸ºè¯å‘é‡ï¼Œä¸¤ä¸ªå‘é‡è¶Šç›¸è¿‘ï¼Œå°±è¡¨ç¤ºä¸¤ä¸ªè¯è¶Šç›¸å…³ï¼Œåä¹‹äº¦ç„¶ã€‚

ä¸æ­¤åŒæ—¶ï¼Œtokenå­—å…¸çš„ç”¨é€”ä¹Ÿå˜æˆäº†ä»è¯å‘é‡çŸ©é˜µä¸­æŸ¥æ‰¾è¯å‘é‡ã€‚å› ä¸ºè¯å‘é‡æ˜¯åµŒå…¥ï¼ˆembeddingï¼‰åˆ°mini-batchçš„ï¼ˆæƒ³æƒ³æŸ¥å­—å…¸ï¼‰ï¼Œå› æ­¤è¯å‘é‡è¯å…¸ä¹Ÿç§°ä¸ºè¯åµŒå…¥çŸ©é˜µï¼ˆword embedding matrixï¼‰ã€‚

```
class ModelDataloader():
  def __init__(self, nums, bs, bptt):
    self.bs, self.bptt = bs, bptt
    self.data = self.batchify(nums)
    self.n = self.data.size(0)

  def __next__(self):
    ......
    
  def batchify(self, data):
    ......

class ModelData():
  def __init__(self, vocab, bs, bptt, trn_ds, val_ds, test_ds=None):
    self.trn_ds, self.val_ds, self.test_ds = trn_ds, val_ds, test_ds
    self.vocab, self.n = vocab, len(vocab)
    self.trn_dl = ModelDataloader(self.trn_ds, bs, bptt)
    self.val_dl = ModelDataloader(self.val_ds, bs, bptt)
    self.test_dl = None if self.test_ds is None else ModelDataloader(self.test_ds, bs, bptt)
```

ModelDataloader.batchify()ç”¨äºå°†æ–‡æœ¬åºåˆ—è½¬æˆçŸ©é˜µï¼ŒModelDataloader.\_\_next__()ç”¨äºè¯»å–mini-batchã€‚

---

## RNN Language Model

![Figure 3: N-gram, language model](https://upload-images.jianshu.io/upload_images/13575947-2a7d4ad12209cdab.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

å¦‚Figure3æ‰€ç¤ºï¼Œè¯­è¨€æ¨¡å‹å°±æ˜¯é€šè¿‡å‰é¢N-1ä¸ªè¯æ¥é¢„æµ‹ç¬¬Nä¸ªè¯ï¼ˆN-gramï¼‰ã€‚

![Figure 4: https://arxiv.org/abs/1611.01462](https://upload-images.jianshu.io/upload_images/13575947-525ff7ed8531b65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Figure4ä¸­ä¸‰æ¡å…¬å¼ç®€æ´æ˜äº†åœ°æè¿°äº†RNNè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼š
- å°†embeddingçŸ©é˜µä¸­çš„è¯å‘é‡åµŒå…¥mini-batchï¼Œå¾—åˆ°$x$ã€‚
- å°†è¾“å…¥$x$å’Œä¸Šä¸€ä¸ª$time$çš„hidden stateï¼ˆ$h_{t-1}$ï¼‰ä¸€èµ·ä¼ å…¥RNNï¼Œå¾—åˆ°å½“å‰$time$çš„hidden stateï¼ˆ$h_{t}$ï¼‰ã€‚
- å°†$h_{t}$ä¼ å…¥åˆ†ç±»å™¨ï¼ˆlinear layerï¼‰ï¼Œå¹¶é€šè¿‡softmaxå‡½æ•°ç”Ÿæˆè¾“å‡ºï¼Œæ¦‚ç‡æœ€å¤§çš„åˆ†ç±»å°±æ˜¯å½“å‰$time$çš„é¢„æµ‹è¯ã€‚

åˆå§‹æ¨¡å‹--RNNLMï¼ˆRNN language modelï¼‰ï¼Œå°±æ˜¯ä¸Šè¿°è¿™æ®µæè¿°çš„æ˜¯ä»£ç å®ç°ï¼š
```
vocab_size = md.n
emb_nf = 200
nf = 256

class RNNLM(nn.Module):
  def __init__(self, vocab_size, emb_nf, nf):
    super().__init__()
    self.vocab_size = vocab_size
    self.emb = nn.Embedding(vocab_size, emb_nf)
    self.i_h = nn.Linear(emb_nf, nf)
    self.rnn = nn.RNN(nf, nf)
    self.h_o = nn.Linear(nf, vocab_size)
    self.h = torch.zeros(1, bs, nf).cuda()
    self.tanh = nn.Tanh()
    
  def forward(self, x):
    x = self.tanh(self.i_h(self.emb(x)))
    o, h = self.rnn(x, self.h)
    self.h = h.detach()
    return F.log_softmax(self.h_o(o), -1).view(-1, self.vocab_size)
```
ä»£ç **â€œself.h = h.detach()â€**çš„æ„æ€æ˜¯ï¼Œä¸‹ä¸€ä¸ªmini-batchçš„è®­ç»ƒå¯ä»¥ä»¥åœ¨å½“å‰mini-batchçš„å­¦ä¹ æˆæœä¸ºåŸºç¡€ã€‚

### Train

```
def validate(stepper, dl, metrics):
      ......
    t = tqdm(iter(dl), leave=False, total=len(dl), miniters=0)
    for xs, y in t:
      preds, loss = stepper.validate(xs, y)
      ......

class Stepper():
  ......
def step(self, xs, y):
    preds = self.m(xs)
    loss = self.crit(preds, y)
    self.opt.zero_grad()
    loss.backward()
    if self.clip: nn.utils.clip_grad_norm_(get_trainable_parameters(m), self.clip)
    self.opt.step()
    return loss.data.item()
  ......

def fit(model, data, epochs, opt, crit, clip=0., metrics=None):
  ......
  for ep in tnrange(epochs, desc='Epochs'):
    ......
    # train
    t = tqdm(iter(data.trn_dl), leave=False, total=len(data.trn_dl), miniters=0)
    for xs, y in t:
      loss = stepper.step(xs, y)
      t.set_postfix(loss=loss, refresh=False)
    t.close()

    # validation
    values = validate(stepper, data.val_dl, metrics)
    ......
```

Trainæ¨¡å—--fit()çš„æ¶æ„å’Œè®¾è®¡ï¼Œå¾ˆå¤§ä¸€éƒ¨åˆ†æˆ‘æ˜¯å‚è€ƒäº†[Fastai library](https://github.com/fastai/fastai/blob/master/old/fastai/model.py)ï¼Œå½“ç„¶ï¼Œæˆ‘ä¹Ÿæ— è€»åœ°ä»ä¸­copyäº†éƒ¨åˆ†ä»£ç ğŸ˜ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰pytorchï¼Œå»ºè®®å…ˆåˆ°pytorchå®˜ç½‘å­¦ä¹ ç›¸å…³æ•™ç¨‹ã€‚

```
fit(m, md, 1, opt, F.nll_loss)

100% 1/1 [19:59<00:00, 1199.91s/it]
     epoch      trn_loss   val_loss  
       0        4.882467   4.941533  
```
åˆ°æ­¤ï¼ŒRNNLMçš„é¦–ç§€ç»“æŸï¼Œä¸€åˆ‡éƒ½è¿è¡Œè‰¯å¥½ã€‚ä½†å®é™…ä¸Šï¼Œfit()å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šæ²¡æœ‰å­¦ä¹ ç‡é€€ç«ï¼ˆlearning rate annealingï¼‰ï¼Œå³åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ä¹ ç‡å›ºå®šä¸å˜ã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œå­¦ä¹ ç‡å†³å®šå°çƒåœ¨ä¼˜åŒ–æ›²çº¿ä¸Šçš„è¿åŠ¨æ­¥è·ï¼Œè¦æƒ³è®©å°çƒè½åœ¨æ›²çº¿æœ€ä½ç‚¹ï¼Œé‚£ä¹ˆåº”è¯¥éšç€è®­ç»ƒè¿‡ç¨‹çš„æ·±å…¥ï¼Œé€æ­¥å‡å°å­¦ä¹ ç‡ï¼Œç¼©å°å°çƒè¿åŠ¨æ­¥è·ï¼ŒåƒFigure3ä¸­çš„ä¸Šå›¾ã€‚åä¹‹ï¼Œå¦‚æœå­¦ä¹ ç‡ä¸å‡åå¢ï¼Œé‚£å°çƒçš„è¿åŠ¨è½¨è¿¹å°±ä¼šåƒFigure3ä¸­ä¸‹å›¾é‚£æ ·ï¼Œåœ¨ä½ç‚¹æ— æ³•æ”¶æ•›ï¼Œåè€Œå‘é«˜ç‚¹åå¼¹ã€‚

![Figure 3](https://upload-images.jianshu.io/upload_images/13575947-cebd833fd9b1c20c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

å­¦ä¹ ç‡å›ºå®šä¸å˜ï¼Œå¯ä»¥é¢„è§åˆ°ï¼Œæ¨¡å‹ä¸å¯èƒ½ä¼šæ”¶æ•›åˆ°æœ€ä½ç‚¹ï¼Œä¼šåœ¨æ›²çº¿ä½ç‚¹é™„è¿‘æ¥å›éœ‡è¡ã€‚å› æ­¤ï¼Œæˆ‘é‡‡ç”¨[SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)æ¥è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºç¯‡å¹…åŸå› æˆ‘å†³å®šæŠŠå®ƒçš„ä»‹ç»æ”¾åˆ°å¦ä¸€ç¯‡åšå®¢ï¼Œå‘åæˆ‘éƒ½æƒ³å¥½äº†ï¼Œ[ä½ åº”è¯¥çŸ¥é“çš„ç¥ç»ç½‘ç»œè®­ç»ƒå¤§æ³•ï¼ˆå¾…å¡«ï¼‰]()ã€‚

å¦‚Figure4æ‰€ç¤ºï¼ŒSGDRä¼šåœ¨æ¯ä¸ªmini-batchè®­ç»ƒç»“æŸåï¼ŒæŒ‰ç…§cosineæ›²çº¿æ¥å‡å°å­¦ä¹ ç‡ï¼Œå¹¶åœ¨æ¯ä¸ªepochå¼€å§‹è®­ç»ƒå‰restartå­¦ä¹ ç‡åˆ°æœ€å¤§å€¼ï¼Œå¹¶å°†å¾…è®­ç»ƒçš„mini-batchæ•°ç¿»å€ã€‚
![Figure 4](https://upload-images.jianshu.io/upload_images/13575947-a87342130466fda9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```
class CosAnneal(LRUpdater):
  ......
  def on_cycle_begin(self):
    super().on_cycle_begin()
    self.t_cur = 0
    
  def calc_lr(self):
    new_lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(self.t_cur / self.nb * np.pi))
    self.t_cur += 1
    if self.t_cur == self.nb:
      self.t_cur = 0
      self.nb *= self.cycle_mult
    return new_lr

def fit(model, data, epochs, opt, crit, clip=0, metrics=None, callbacks=None):
  ......
      for xs, y in t:
        for cb in callbacks: cb.on_batch_begin()
        loss = stepper.step(xs, y)
        for cb in callbacks: cb.on_batch_end(loss)
    ......
```

CosAnnealå°±æ˜¯SGDRçš„å­¦ä¹ ç‡é€€ç«å‡½æ•°ï¼Œå®ƒä¼šåœ¨æ¯ä¸ªmini-batchè®­ç»ƒç»“æŸåï¼ˆcb.on_batch_end()ï¼‰è°ƒæ•´å­¦ä¹ ç‡ã€‚

```
fit(m, md, epochs, opt, F.nll_loss, callbacks=[cosanneal])

100% 2/2 [1:00:04<00:00, 1560.49s/it] 
epoch      trn_loss   val_loss  
       0        4.965671   5.012351       
       1        4.839868   4.848449  
       1        4.779568   4.7841 
```

å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹å·²ç»å¯ä»¥æ”¶æ•›äº†ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆå€¾å‘ã€‚æ¯ä¸ªepochçš„è®­ç»ƒç»“æœï¼Œéƒ½æ˜¯train losså°äºvalidation lossï¼Œé‚£è‡³å°‘è¯´æ˜æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

åˆ°æ­¤ï¼ŒRNNLMè¯¥é€€åœºäº†ï¼Œæ¥ä¸‹æ¥æ—¶é—´äº¤ç»™æˆ‘ä»¬å¦ä¸€ä¸ªä¸»è§’--weight-dropped LSTMã€‚

## weight-dropped LSTM

[weight-dropped LSTM](https://arxiv.org/abs/1708.02182)çš„ä½œè€…Stephen Merityæå‡ºäº†ä¸€ä¸ªæ´è§ï¼šå°†CNNçš„æ ‡é…ï¼Œä¸“ç”¨äºå¯¹æŠ—è¿‡æ‹Ÿåˆçš„dropoutåº”ç”¨äºRNNæ¨¡å‹ã€‚è¿™å°±æ˜¯weight-dropped LSTMã€‚

weight-dropped LSTMå’ŒRNNLMçš„åŒºåˆ«åœ¨äºï¼Œé™¤äº†éšå±‚æ˜¯ç”±3ä¸ªLSTMå±‚ç»„æˆä¹‹å¤–ï¼Œæ¨¡å‹ä¸­çš„å„å±‚éƒ½å¢åŠ äº†dropoutã€‚è¿™æ®µä»£ç å¤ªé•¿ï¼Œæˆ‘å°±ä¸è´´äº†ï¼Œ[è¯·ç‚¹è¿™é‡ŒæŸ¥çœ‹](https://github.com/alexshuang/writingbot/blob/master/writingbot.ipynb)ã€‚

æ¨¡å‹ä¸­çš„dropoutæœ‰ä¸‰ç§ï¼š
- **embbeding dropout**ï¼šé¡¾åæ€ä¹‰ï¼Œå®ƒæ˜¯ä½œç”¨äºembeddingçŸ©é˜µçš„dropoutï¼Œå®ƒé™¤äº†dropoutåŠŸèƒ½ä¹‹å¤–ï¼Œè¿˜ä¼šå°†non-dropped-outå‘é‡scaleä¸ºå®ƒçš„$1/(1-p)$å€ï¼Œpæ˜¯dropoutçš„æ¦‚ç‡ã€‚
- **variational dropout**ï¼šå®ƒä½œç”¨äºå±‚ä¸å±‚ä¹‹é—´ï¼Œä½œç”¨äºhidden stateï¼Œæ¢å¥è¯è¯´ï¼Œå®ƒæŠ›å¼ƒæ‰äº†ä¸€ä¸ªmini-batchä¸­æŸäº›è¯çš„å­¦ä¹ ç»“æœï¼ˆhidden stateï¼‰ã€‚è·Ÿembbeding dropoutä¸€æ ·ï¼Œéƒ½æ˜¯ä»tokençš„å±‚é¢æ¥è¿›è¡Œdropoutã€‚
- **weight dropout**ï¼šå®ƒä½œç”¨äºLSTMå±‚ï¼Œä½œç”¨äºLSTMå±‚å†…éƒ¨çš„ç¥ç»å…ƒæƒå€¼ã€‚è¿™é‡Œä¸æ˜¯ç›´æ¥ä¿®æ”¹pytorch LSTMç±»çš„ä»£ç ï¼Œè€Œæ˜¯é€šè¿‡åˆ›å»ºLSTMçš„wrapperç±»--WeightDropï¼Œç”±å®ƒé€šè¿‡å§”æ‰˜ï¼ˆdelegateï¼‰æ¥è°ƒç”¨LSTM.forward()ï¼Œå¦‚æ­¤å°±å¯ä»¥æ›²çº¿å¯¹LSTMå†…éƒ¨çš„æƒå€¼åšdropoutã€‚

é™¤äº†dropoutï¼Œè¿™ç¯‡[paper](https://arxiv.org/abs/1708.02182)åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿˜ç”¨åˆ°ä¸€ä¸ªå¾ˆåˆ›æ–°çš„ç‚¹ï¼šBPTTï¼ˆBack Propagation Through Timeï¼‰ã€‚çœ¼ç†Ÿå§ï¼Œå®ƒåœ¨å‰é¢**Dataset**æ¨¡å—ä¸­æåˆ°ï¼Œä¸è®°å¾—çš„å¯ä»¥å›Figure1çœ‹ä¸€çœ¼ã€‚

æ–‡æœ¬æ•°æ®çš„ä¸¤ä¸ªç‰¹æ€§ï¼šé¡ºåºæ€§å’Œå›ºå®šçš„è¯­æ³•ç»“æ„ï¼Œä½¿å¾—RNNæ—¢ä¸èƒ½åƒCNNé‚£æ ·ï¼Œå¯ä»¥æ‰“ä¹±è®­ç»ƒæ ·æœ¬çš„æ’åˆ—é¡ºåºï¼ˆshuffleï¼‰ï¼Œä¹Ÿä¸èƒ½è½»æ˜“åœ°åšæ•°æ®æ‰©å……ï¼ˆdata augmentationï¼‰ã€‚è¿™æ ·ä¸€æ¥ï¼Œå–‚ç»™RNNçš„æ•°æ®å°±ç¼ºä¹äº†å˜åŒ–å’Œéšæœºï¼Œä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

ä¸ºæ­¤ï¼Œweight-dropped LSTMåœ¨è¯»å–mini-batchæ—¶ï¼Œseq_lenä¸æ˜¯å›ºå®šçš„ï¼Œå®ƒæœ‰95%çš„æ¦‚ç‡è½åœ¨$\mu=bpttï¼ˆè¶…å‚ï¼‰, \sigma=5$çš„åˆ†å¸ƒä¸­ï¼Œå‡è®¾bptt=70ï¼Œé‚£ä¹ˆseq_lenæœ‰90%çš„æ¦‚ç‡è½åœ¨[55ï¼Œ85]ã€‚å¦å¤–è¿˜æœ‰5%çš„æ¦‚ç‡ï¼Œbpttä¼šå‡åŠï¼Œå³bptt/2ã€‚

```
emb_sz = 200
nf = 500
nl = 3
clip = 0.25
ps = np.array([0.05, 0.05, 0.05, 0.02, 0.1]) * 1
lr = 3e-3
cycle_len = 1
cycle_mult = 2
epochs = 3
epochs = [cycle_len * cycle_mult**i for i, _ in enumerate(range(epochs))]

m = WDLSTM(bs, vocab_sz, emb_sz, pad_idx, nf, nl, ps).cuda()
opt = optim.Adam(m.parameters(), lr=lr, betas=(0.75, 0.99))
cosanneal = CosAnneal(opt, len(md.trn_dl), cycle_len=cycle_len, cycle_mult=cycle_mult)
fit(m, md, epochs, opt, F.cross_entropy, clip=clip, callbacks=[cosanneal], metrics=[accuracy])

HBox(children=(IntProgress(value=0, description='Epochs', max=3, style=ProgressStyle(description_width='initiaâ€¦
     epoch      trn_loss   val_loss   accuracy  
       0        4.882206   4.794618   0.239594  
       1        4.812892   4.651131   0.247563  
       1        4.529045   4.584595   0.254397  
       2        4.523168   4.593299   0.250682  
       2        4.562455   4.524473   0.256603  
       2        4.374611   4.481817   0.261064  
       2        4.486718   4.47125    0.262309 
```
å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œæ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«äº†ï¼Œè¿‡æ‹Ÿåˆä¹Ÿæ¶ˆå¤±äº†ã€‚æ›´æƒŠå–œçš„æ˜¯ï¼Œè™½ç„¶æ¨¡å‹çš„å¤æ‚åº¦æˆå€å¢åŠ äº†ï¼Œä½†è®­ç»ƒæ—¶é—´å´å¹¶æ²¡æœ‰å› æ­¤å¢åŠ ã€‚æ¨¡å‹å®Œç¾åœ°å±•ç°äº†æ·±åº¦ç¥ç»ç½‘ç»œå¯¹æŠ—è¿‡æ‹Ÿåˆçš„æ­£ç¡®å§¿åŠ¿ï¼š**å¢åŠ æ­£åˆ™åŒ–ï¼ˆdropoutå’Œweight decayï¼‰ï¼Œè€Œéé™ä½ç¥ç»ç½‘ç»œå¤æ‚åº¦**ã€‚

## Testing

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†æ¨¡å‹çš„åˆæ­¥è®­ç»ƒï¼Œæ¥ä¸‹æ¥å¼€å§‹è¿›è¡Œæ¨¡å‹æµ‹è¯•ã€‚æˆ‘åœ¨è±†ç“£ç”µå½±ä¸Šéšæ„é€‰äº†å‡ å¥å½±è¯„ï¼Œç»æœ‰é“è¯å…¸ç¿»è¯‘åä¸¢ç»™æ¨¡å‹ï¼Œè®©å®ƒå¾ªç¯ç”Ÿæˆæœ€é•¿ä¸º500è¯çš„å†…å®¹ã€‚ä¸ºç®€åŒ–æ¼”ç¤ºï¼Œé»˜è®¤åªè¾“å‡ºç”Ÿæˆçš„ç¬¬ä¸€ä¸ªå¥å­ï¼Œå¦‚æœéœ€è¦è¾“å‡ºæ›´å¤šçš„æ–‡æœ¬ï¼Œå¯ä»¥é€šè¿‡å‚æ•°**sentences**æŒ‡å®šã€‚

```
s = 'God is really the reversal of the goose bumps'
res = keep_writing(m, s, sentences=2)
print("> ", s)
print("Bot: ", res)

>  God is really the reversal of the goose bumps
Bot:  i think that the movie is a bit too long, but it is a very good movie. i have to say that i was surprised to see that this movie was made in the same vein as "the blair witch project". 
```

å¯ä»¥çœ‹åˆ°ï¼Œæ­£å¦‚æ–‡ç« å¼€å¤´æ‰€æè¿°çš„é‚£æ ·ï¼Œæœºå™¨äººç”Ÿæˆçš„å†…å®¹æ²¡æœ‰è¯­æ³•ç¡¬ä¼¤ï¼Œæ ‡ç‚¹ç¬¦å·ä¹Ÿç”¨çš„ç²¾ç¡®ï¼ˆå°¤å…¶æ˜¯åŒå¼•å·ï¼‰ï¼Œè™½ç„¶æ²¡æœ‰ä»€ä¹ˆæ–‡é‡‡ï¼Œä½†å¥½æ­¹ä¸»é¢˜æ²¡è·‘åã€‚

æ€»çš„æ¥è¯´ï¼Œå¯¹äºè¿™ä¸ªç»“æœæˆ‘è¿˜æ˜¯åŸºæœ¬æ»¡æ„çš„ï¼Œæ¥ä¸‹è¦åšçš„å°±æ˜¯ç»§ç»­æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚é™¤äº†ç»§ç»­è®­ç»ƒæ¨¡å‹ç›´åˆ°è¿‡æ‹Ÿåˆä¹‹å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡é¢„è®­ç»ƒå’Œè¿ç§»è®­ç»ƒæ¥ç»™æ¨¡å‹èµ‹èƒ½ã€‚

## Pretrain: fasttext

æˆ‘ç›¸ä¿¡ä½ ä¸€å®šå¬è¯´è¿‡é¼é¼å¤§åçš„word2vocï¼Œå®é™…ä¸Šï¼Œå®ƒå°±æ˜¯embeddingçŸ©é˜µï¼Œåªæ˜¯å®ƒç»è¿‡åœ¨wikipediaç­‰å¤§å‹è¯­æ–™åº“çš„è®­ç»ƒåï¼Œè¯å‘é‡ä¸å†æ˜¯éšæœºå€¼ã€‚æ¢å¥è¯è¯´ï¼Œword2vocçš„é¢„è®­ç»ƒå·²ç»å¸®æ¨¡å‹å­¦ä¼šäº†è‹±æ–‡å•è¯ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå†å­¦ä¹ è‹±æ–‡çš„è¯­æ³•ï¼Œæ•ˆæœè‡ªç„¶æ˜¯äº‹åŠåŠŸå€ã€‚

ç›¸æ¯”å¤è€çš„word2vocï¼Œfasttextæ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œå®ƒç»è¿‡æ›´å¤šæ›´å¤§çš„è¯­æ–™åº“è®­ç»ƒï¼Œæ”¯æŒçš„è¯­è¨€ä¹Ÿæ›´å¤šã€‚

ä¸çŸ¥é“ä½ å‘ç°æ²¡æœ‰ï¼Œä¸ç®¡æ˜¯word2vocè¿˜æ˜¯fasttextï¼Œéƒ½æœ‰ä¸ªå¼Šç«¯ï¼šå®ƒä»¬åªæ˜¯è®­ç»ƒäº†embeddingçŸ©é˜µï¼Œè€Œå®ƒåªå æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†ã€‚ä¸è¿™ç§è®­ç»ƒç›¸åçš„æ˜¯è¿ç§»è®­ç»ƒï¼Œå³å…ˆç”¨ç›¸åŒçš„è¯­æ–™åº“æ¥è®­ç»ƒæ¨¡å‹ï¼Œå†ç”¨IMDBè¯­æ–™åº“æ¥fine-tuneæ¨¡å‹ï¼Œè¿™æ ·çš„æ•ˆæœæœ€å¥½ï¼Œä½†æ•´ä¸ªè®­ç»ƒå°±éœ€è¦èŠ±è´¹å¾ˆé•¿å¾ˆé•¿çš„æ—¶é—´ã€‚

æ­£æ‰€è°“è¿›ä¸€å¯¸æœ‰ä¸€å¯¸çš„æ¬¢å–œï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆç”¨fasttexté¢„è®­ç»ƒæ¥æ„Ÿå—ä¸‹è¿›æ­¥çš„å–œæ‚¦ã€‚

[Fasttext](https://fasttext.cc/)æä¾›äº†ä¸¤ç§æ•°æ®æ ¼å¼ï¼šbinã€txtï¼Œå‰è€…çš„åŠ è½½é€Ÿåº¦æ¯”åè€…å¿«å¾—å¤šï¼Œå¦‚æœä½ æœºå™¨å†…å­˜è¶³å¤Ÿï¼Œå»ºè®®ç”¨å‰è€…ã€‚å¦‚æœä½ æœ‰å†…å­˜ä¸è¶³çš„å›°æ‰°ï¼Œå¯ä»¥ä»txtæ–‡ä»¶è¯»å–ï¼Œä½†ç”¨æ—¶ä¼šæ¯”å‰è€…è¦é•¿å¾—å¤šã€‚

```
m = WDLSTM(bs, vocab_sz, emb_sz, pad_idx, nf, nl, ps, emb_weights=emb_w).cuda()
fit(m, md, epochs, opt, F.cross_entropy, clip=clip, callbacks=[cosanneal], metrics=[accuracy])

HBox(children=(IntProgress(value=0, description='Epochs', max=3, style=ProgressStyle(description_width='initiaâ€¦
     epoch      trn_loss   val_loss   accuracy  
       0        4.579641   4.529013   0.252729  
       1        4.548646   4.438005   0.260425  
       1        4.379018   4.365056   0.267696  
       2        4.414107   4.4228     0.261393  
       2        4.526043   4.361377   0.267618  
       2        4.145617   4.318527   0.272548  
       2        4.295489   4.305556   0.273613  
```

å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡åï¼Œç»è¿‡ç›¸åŒçš„è®­ç»ƒï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†1%ï¼Œæ•ˆæœä¸é”™ã€‚

### END

æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ç”¨è¯­è¨€æ¨¡å‹ä»é›¶æ‰“é€ IMDBå½±è¯„æœºå™¨äººã€‚è¯­è¨€æ¨¡å‹ç»™æˆ‘çš„æ„Ÿè§‰æ˜¯ï¼Œè®­ç»ƒæˆæœ¬å¾ˆä½ï¼Œä¸éœ€è¦å¯¹æ•°æ®åšæ ‡æ³¨ï¼Œéšä¾¿æ¢ä¸ªæ•°æ®é›†å°±å¯ä»¥å˜èº«ä¸ºAIxxå†™ä½œæœºå™¨äººã€‚æˆ‘ä¸‹ä¸€ç¯‡åšå®¢è¦ç”¨seq2seqæ¨¡å‹å¼€å‘èŠå¤©æœºå™¨äººï¼Œå±Šæ—¶å†æ¥å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ£ï¼Œå¸Œæœ›èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–è¯¥æ¨¡å‹ã€‚
